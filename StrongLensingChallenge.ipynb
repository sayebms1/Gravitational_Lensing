{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong Lensing Challenge\n",
    "\n",
    "This is an example notebook for the Strong Lensing Challenge. In this notebook, we present a simple CNN model implemented using the PyTorch library to solve the task of binary classification of strong lensing images. We then implement an unsupervised model for anomaly detection which is the primary objective of this challenge.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The Dataset consists of two classes, strong lensing images with no substructure and strong lensing images with substructure. Considering the samples with substructure to be outliers, you are required to train an unsupervised model on a set of strong lensing images with no substructure to solve the task of anomaly detection.\n",
    "\n",
    "Link to the Dataset: https://github.com/ML4SCI-SLC/SLC_Data\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "* Distribution of Reconstruction Loss,\n",
    "* ROC curve (Receiver Operating Characteristic curve) and AUC score (Area Under the ROC Curve)   \n",
    "\n",
    "The model performance will be tested on the hidden test dataset based on the above metrics.\n",
    "\n",
    "### Instructions for using the notebook\n",
    "\n",
    "1. Use GPU acceleration: (Edit --> Notebook settings --> Hardware accelerator --> GPU)\n",
    "2. Run the cells: (Runtime --> Run all)\n",
    "\n",
    "> **_NOTE:_**  This notebook contains a section for a supervised approach for binary classification, this is only for the sake of intuition, and you are required to work only on the unsupervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Dataset\n",
    "!git clone https://github.com/ML4SCI-SLC/SLC_Data.git\n",
    "%cd SLC_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary Classification using a Supervised Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Visualization and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "%matplotlib inline\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "path_sub = './lenses/sub' # Path to samples with substructure \n",
    "files_sub = [os.path.join(path_sub, f) for f in os.listdir(path_sub) if f.endswith(\".jpg\")]\n",
    "random.shuffle(files_sub)\n",
    "\n",
    "# Number of samples to display per class\n",
    "n = 5\n",
    "\n",
    "# Plot the samples with substructure \n",
    "print('Samples with substructure: ')\n",
    "i = 1\n",
    "plt.rcParams['figure.figsize'] = [14, 14]\n",
    "for image in files_sub[:n]:\n",
    "    ax = plt.subplot(2,n,i)\n",
    "    plt.imshow(cv2.imread(image,0), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    i += 1\n",
    "plt.show()\n",
    "    \n",
    "path_nosub = './lenses/no_sub' # Path to samples with no substructure \n",
    "files_nosub = [os.path.join(path_nosub, f) for f in os.listdir(path_nosub) if f.endswith(\".jpg\")]\n",
    "random.shuffle(files_nosub)\n",
    "\n",
    "# Plot the samples with no substructure \n",
    "print('Samples with no substructure: ')\n",
    "i = 1\n",
    "for image in files_nosub[:n]:\n",
    "    ax = plt.subplot(2,n,i+n)\n",
    "    plt.imshow(cv2.imread(image,0), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation \n",
    "transform = transforms.Compose([transforms.Grayscale(),\n",
    "                                        transforms.Resize((150, 150)),\n",
    "                                        transforms.RandomAffine(degrees=(0,90), translate=(0.2,0.2)),\n",
    "                                        transforms.ToTensor()])\n",
    "\n",
    "batch_size = 100\n",
    "data_path = './lenses' # Path to input data\n",
    "data = datasets.ImageFolder(data_path, transform = transform)\n",
    "print(\"Classes: \" + str(data.class_to_idx))\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for samples, labels in data_loader:\n",
    "    x.append(samples.numpy())\n",
    "    y.append(labels.numpy())\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print('Data Processed!')\n",
    "print('Shape of the Data: ' + str(x.shape))\n",
    "print('Shape of the labels: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defining a CNN Model\n",
    "\n",
    "You may refer to this [article](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148) to learn about Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, (11,11), stride=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (11,11), stride=3)\n",
    "        self.pooling = nn.MaxPool2d(2,2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out validation\n",
    "split = 0.8\n",
    "x_train = x[:int(x.shape[0]*split)]\n",
    "y_train = y[:int(x.shape[0]*split)]\n",
    "\n",
    "n_epochs = 10 # Number of Epochs\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-5)\n",
    "\n",
    "# Train the model\n",
    "pbar = tqdm(range(1, n_epochs+1))\n",
    "loss_array = []\n",
    "for epoch in pbar:\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for i in range(len(x_train)):\n",
    "        samples = torch.tensor(x_train[i], dtype=torch.float32, device=device)\n",
    "        labels = torch.tensor(y_train[i], dtype=torch.long, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples)\n",
    "        _, preds = torch.max(model(samples).data, 1)\n",
    "        correct = (preds == labels).float().sum()\n",
    "        loss = criteria(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += correct/batch_size\n",
    "\n",
    "    train_loss = train_loss/len(data)\n",
    "    loss_array.append(train_loss)\n",
    "    \n",
    "    pbar.set_postfix({'Epoch': epoch, 'Training Loss': train_loss, 'Training Accuracy': train_acc.cpu().numpy()/x_train.shape[0]})\n",
    "    torch.save(model, 'CNN_model.pth')\n",
    "    \n",
    "# Plot the training loss\n",
    "plt.rcParams['figure.figsize'] = [7, 5]\n",
    "plt.plot(loss_array)\n",
    "plt.title('Training Loss Convergence')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Testing the CNN Model\n",
    "\n",
    "You may refer to this [article](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152) to learn about AUC ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# Validation samples\n",
    "x_val = x[int(x.shape[0]*split):].reshape(-1,1,150,150)\n",
    "y_val = y[int(x.shape[0]*split):].reshape(-1,1)\n",
    "\n",
    "val_data = torch.from_numpy(x_val)\n",
    "if torch.cuda.is_available():\n",
    "  val_data = val_data.cuda()\n",
    "probs = torch.nn.functional.softmax(model(val_data), dim=1)\n",
    "auc_score = roc_auc_score(y_val, probs[:,-1].cpu().detach().numpy())\n",
    "\n",
    "print('Validation AUC: ' + str(auc_score))\n",
    "\n",
    "# Calculate AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_val, probs[:,-1].cpu().detach().numpy())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the AUC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.5f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Binary Classification')\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this notebook presents a supervised approach, the goal of this challenge is to design and implement an unsupervised machine learning methodology for anomaly detection. We use the distribution of reconstruction loss and the area under the ROC curve (AUC) for Classification based on the reconstruction loss as metrics for evaluating the performance of the unsupervised model.\n",
    "\n",
    "You must only submit the unsupervised implementation. The use of any supervised models will not be considered for evaluation. \n",
    "\n",
    "## 2. Anomaly Detection using an Unsupervised Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes: \" + str(data.class_to_idx))\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size = 1, shuffle=True)\n",
    "\n",
    "no_sub = []\n",
    "sub = []\n",
    "for samples, labels in data_loader:\n",
    "    if labels[0] == 0:\n",
    "        no_sub.append(samples.numpy()[0])\n",
    "    else:\n",
    "        sub.append(samples.numpy()[0])\n",
    "no_sub = np.asarray(no_sub)\n",
    "sub = np.asarray(sub)\n",
    "\n",
    "no_sub = no_sub.reshape(batch_size,-1,1,150,150)\n",
    "sub = sub.reshape(batch_size,-1,1,150,150)\n",
    "\n",
    "# Hold-out validation\n",
    "split = 0.8\n",
    "x_train = no_sub[:int(no_sub.shape[0]*split)]\n",
    "\n",
    "print(\"Shape of no substructure data: \" + str(no_sub.shape))\n",
    "print(\"Shape of substructure data: \" + str(sub.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining an RBM model\n",
    "\n",
    "You may refer to this [article](https://medium.com/datadriveninvestor/an-intuitive-introduction-of-restricted-boltzmann-machine-rbm-14f4382a0dbb) to learn about Restricted Boltzmann Machine (RBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RBM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_vis=22500, n_hid=1000, k=1):\n",
    "        super(RBM, self).__init__()\n",
    "        self.v = nn.Parameter(torch.randn(1, n_vis))\n",
    "        self.h = nn.Parameter(torch.randn(1, n_hid))\n",
    "        self.W = nn.Parameter(torch.randn(n_hid, n_vis))\n",
    "        self.k = k\n",
    "\n",
    "    def visible_to_hidden(self, v):\n",
    "        p = torch.sigmoid(F.linear(v, self.W, self.h))\n",
    "        return p.bernoulli()\n",
    "\n",
    "    def hidden_to_visible(self, h):\n",
    "        p = torch.sigmoid(F.linear(h, self.W.t(), self.v))\n",
    "        return p.bernoulli()\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        v_term = torch.matmul(v, self.v.t())\n",
    "        w_x_h = F.linear(v, self.W, self.h)\n",
    "        h_term = torch.sum(F.softplus(w_x_h), dim=1)\n",
    "        return torch.mean(-h_term - v_term)\n",
    "\n",
    "    def forward(self, v):\n",
    "        h = self.visible_to_hidden(v)\n",
    "        for _ in range(self.k):\n",
    "            v_gibb = self.hidden_to_visible(h)\n",
    "            h = self.visible_to_hidden(v_gibb)\n",
    "        return v, v_gibb, h\n",
    "\n",
    "model = RBM().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training the RBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 2e-3, epochs=n_epochs, steps_per_epoch=x_train.shape[0])\n",
    "\n",
    "# Train the model\n",
    "loss_array = []\n",
    "for epoch in tqdm(range(1, n_epochs+1)):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "\n",
    "        data = torch.from_numpy(x_train[i])\n",
    "        if torch.cuda.is_available():\n",
    "          data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        v, v_gibbs, hidden = model(data.view(-1, 22500))\n",
    "        loss = model.free_energy(v) - model.free_energy(v_gibbs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # print avg training statistics\n",
    "    train_loss = train_loss/x_train.shape[0]\n",
    "    loss_array.append(train_loss)\n",
    "    \n",
    "# Plot the training loss\n",
    "plt.rcParams['figure.figsize'] = [7, 5]\n",
    "plt.plot(loss_array)\n",
    "plt.title('Training Loss Convergence')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Testing the RBM Model\n",
    "\n",
    "You may refer to this [article](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152) to learn about AUC ROC Curve\n",
    "\n",
    "Additionally, you may also refer to this [article](https://towardsdatascience.com/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098) to learn how the AUC score is calculated in the case of Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Validation samples\n",
    "x_test1 = no_sub[int(no_sub.shape[0]*split):]\n",
    "x_test2 = sub[int(sub.shape[0]*split):]\n",
    "\n",
    "# Get the reconstructed samples\n",
    "out1 = []\n",
    "out2 = []\n",
    "for i in range(x_test1.shape[0]):\n",
    "\n",
    "        data1 = torch.from_numpy(x_test1[i])\n",
    "        data2 = torch.from_numpy(x_test2[i])\n",
    "        if torch.cuda.is_available():\n",
    "          data1 = data1.cuda()\n",
    "          data2 = data2.cuda()\n",
    "        v1, v_gibbs1, hidden1 = model(data1.view(-1, 22500))\n",
    "        v2, v_gibbs2, hidden2 = model(data2.view(-1, 22500))\n",
    "        out1.append(v_gibbs1.cpu().detach().numpy())\n",
    "        out2.append(v_gibbs2.cpu().detach().numpy())\n",
    "\n",
    "no_sub_recon = np.asarray(out1)\n",
    "sub_recon = np.asarray(out2)\n",
    "\n",
    "x_test1 = x_test1.reshape(-1,22500)\n",
    "x_test2 = x_test2.reshape(-1,22500)\n",
    "no_sub_recon = no_sub_recon.reshape(-1,22500)\n",
    "sub_recon = sub_recon.reshape(-1,22500)\n",
    "\n",
    "# Calculate reconstruction loss\n",
    "losses1 = []\n",
    "losses2 = []\n",
    "for i in range(x_test1.shape[0]):\n",
    "    \n",
    "    criteria = nn.MSELoss()\n",
    "    losses1.append(criteria(torch.from_numpy(x_test1[i]), torch.from_numpy(no_sub_recon[i])))\n",
    "    losses2.append(criteria(torch.from_numpy(x_test2[i]), torch.from_numpy(sub_recon[i])))\n",
    "\n",
    "losses1 = np.asarray(losses1)\n",
    "losses2 = np.asarray(losses2)\n",
    "\n",
    "# Plot the distribution of reconstruction loss\n",
    "w = 0.0005\n",
    "n1 = math.ceil((losses1.max() - losses1.min())/w)\n",
    "n2 = math.ceil((losses2.max() - losses2.min())/w)\n",
    "\n",
    "sns.distplot(losses2,label='sub',kde=0,norm_hist=0, color=\"r\", bins=n2)\n",
    "sns.distplot(losses1,label='no_sub',kde=0,norm_hist=0, color=\"g\", bins=n1)\n",
    "plt.legend()\n",
    "plt.xlabel('MSELoss')\n",
    "plt.ylabel('Population')\n",
    "plt.title('Distribution of Reconstruction Loss for test samples')\n",
    "plt.show()\n",
    "\n",
    "y_ts1 = []\n",
    "for i in range(losses1.shape[0]):\n",
    "    y_ts1.append(0)\n",
    "y_ts1 = np.asarray(y_ts1)\n",
    "y_ts2 = []\n",
    "for i in range(losses2.shape[0]):\n",
    "    y_ts2.append(1)\n",
    "y_ts2 = np.asarray(y_ts2)\n",
    "\n",
    "x_ts = np.concatenate((losses1,losses2)).reshape(-1,1)\n",
    "y_ts = np.concatenate((y_ts1,y_ts2))\n",
    "\n",
    "# Calculate AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_ts, x_ts)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the AUC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.5f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Binary Classification')\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score is very low since we have trained a basic RBM model which is chosen for the sole purpose of demonstration. We expect your model to have a high AUC score. One interesting approach could be the use of autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guidelines \n",
    "\n",
    "* You are required to submit a Google Colab Jupyter Notebook clearly showing your implementation along with the evaluation metrics (Distribution of Reconstruction Loss, ROC curve, and AUC score) for the training and validation data.\n",
    "* You must also submit the final trained model, including the model architecture and the trained weights ( For example: HDF5 file, .pb file, .pt file, etc. )\n",
    "* You can use this example notebook as a template for your work. \n",
    "* As mentioned earlier, you are only required to submit the unsupervised implementation.  \n",
    "\n",
    "> **_NOTE:_**  You are free to use any ML framework such as PyTorch, Keras, TensorFlow, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
